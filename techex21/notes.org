* talk
  TSS integration is based on the principle of,
  you need to be able to run your software

  we couldn't run our software before;
  we could run unit tests,
  and we could maybe run one or a few processes together against a bunch of mocks,
  and we could laboriously configure a bunch of resources to create a "QA system" by copying production
  but we couldn't run the actual system as it was actually intended

  but, once we can run our system,
  well, besides the most obvious use, using it in production,
  the second most obvious use is to run it to test it.

  (er...)

  anyway, so how do we manage to get our system running?
  we have a bunch of processes with complex dependencies,
  and we need to run them.

  this task is actually pretty simple!
  the hard part is understanding how your system actually works,
  but once you've done that;
  this knowledge can be encoded in a regular program.

  [slide(s) about each of the points of the basic type system system stuff]

  OK, that's fairly abstract.
  Let's look at a concrete example now.

  We'll make a test for a specific system.

  I promise you, it doesn't matter what this system does.
  I chose it as an example because it has moderately complicated dependencies,
  but not too complicated.

  The actual complex part, understanding the system,
  has already been done and encoded into the type system by a domain expert.
** building test in the talk
   okay so... the iso validator seems like a good candidate since it depends on marketdata,
   and is reasonably complex yet simple...

   but testing it, is a bit tricky...
   since we need a bump or whatever. hm.

   I suppose bump might not be *that* complex?

   ugh, disseminator... what a complication.

   ok so what's the list

   location, posdelta, disseminator -> hfmd -> feeds, tss_control, venues -> venue sim, static data, fastpath

   tss_control, kinda sucks.

   the rest of these... hmm...

   okay I guess tss_control doesn't suck, it's a real input that WMM uses too...

   posdelta is basically an in-out, a bus to register ourselves on...

   disseminator, sigh, is an annoying indirection, but fine, it's marketdata.

   static_data, ok that's real,

   same with venues and fastpath.
   and location.

   hm.

   I dunno, it's not actually all that complicated I guess

   although if we include the manual trader, then it is complicated, for sure.

   hm. maybe showing bit by bit is kind of irrelevant though.
   I mean, I want to impress with the ease,
   not show the real complexity in this system.

   hmm but if we do include the manual trader it would be so delightfully complete...

   eh... maybe a marketdata example is simpler.
   like, just HFMD

   right, we make staticdata... hmm RegNMSSimulator is a bit complex...

   what about TPS, maybe?

   that one is nice because we can demonstrate that the same code is used to start it in prod!

   hmm and run_tps is pretty simple too.

   yeah this also demonstrates remote usage.

   ok I think TPS is probably the best example.

   we should... porbably clean up some weird stuff that we did.
*** lnc
    hey, how about we just mirror lnc_library in the monorepo?
    treat the HG repo as canonical still, but just mirror it in the monorepo,
    so we can do tests against it?
*** cleaning up TPS
   like, I don't think expecting stuff in CWD is a good idea, anymore...
   I mean... it's just so awkward...
   passing them separately instead of as a bundle is nicer;
   bundling them requires, uh, "consing" a "pair" - making the directory and the symlinks

   a hassle.

   ok so like,
   passing the fds would be even nicer,
   buuuuuuuuuuuuuuuut,
   we don't deal with them as FDs right now so passing them as paths is a smidgen easier

   hm. the ib to tps subaccount mapping.
   it would be nice to pass those... in a more...
   type-y way. hmmm....
   on stdin, maybe...?

   getting rid of "location" entirely would be really nice yeah...

   oh hey. why not just pass it as JSON on the command line?
   as one argument.

   that would be... super easy!

   hmm okay so how do we do persistence if we aren't enforcing a certain layout...
   I guess start_persisting_tps is what enforces it, hmm...

   I guess, there's no real reason to use a symlink now?
   nor ever?
   like, a directory containing some symlinks,
   is no better than a file containing some paths.

   but then again, a file containing some paths is no better than a directory containing some symlinks.

   except, well, that a directory can't be updated atomically... we need the symlink instead.

   and... also I can't use flink stuff sigh
   no linkat with O.TMPFILE

   I guess I can do it with proc, fine.

   although... I do like the...
   fact that the...
   symlinked directories... stay around...

   so it's nice to have all these old rundirs.
*** outline of building test in the talk
    ok so we'll show the method signature of start_tps

    we'll talk about how this is common between prod and test..

    then... run through each argument,
    producing the thing we need...

    hmmm
* real talk

** Who is this guy?
Name: Spencer Baugh
Team: Modern Infrastructure for Trading Systems (MITS)
Hat: Two Sigma Securities (TSS)

Talk is about:
Testing in TSS with the "TSS Integration" library

1. Brief mundane details of TSS Integration
2. Philosophical discussion of testing (hopefully novel)
3. In-depth example of writing a test with TSS Integration
4. Recap of the TSS mindset

If something isn't making sense, please ask;
give me the chance to clear it up.

Let's begin!

** Brief mundane details

- TSS Integration is a Python library
- in the monorepo at ts/tss/integration

- Used by teams in TSS and TSI
- Maintained collectively by those teams
- Originally developed by the MITS team in TSS

- Uses Python type annotations:
#+begin_src python
var: Type = Type()

def f(arg: Type) -> ReturnType: ...
#+end_src

- Uses async Python:
#+begin_src python
async def f(arg: Type) -> ReturnType:
    x = await g(arg)
    await h(x)
    ...
#+end_src

(We use the open source "trio" library for async)

It's a normal Python library.

** Philosophical discussion (hopefully novel)

First off:

- TSS Integration is not a "testing library"
- Its purpose is not testing
- It's not even really testing-related

TSS Integration provides this function:

*The ability to run the trading system*

(See http://catern.com/run.html)

*The ability to run the trading system*

Two Sigma systems are:
- made up of many different components,
- many different services,
- provided by many different teams.

*The ability to run the trading system*

TSS Integration:
- Runs these components
- Connects these services
- In arbitrary user-controlled configurations 
- (Configurations are not predetermined by the library!)

*The ability to run the trading system*

| Use case        | Do we actually, for real, do it with TSS Integration today? |
|-----------------+-------------------------------------------------------------|
| Experimentation | Yes                                                         |
| Development     | Yes                                                         |
| Testing         | Yes                                                         |
| Profiling       | Yes                                                         |
| Production      | For several important subsystems (more soon?)               |

*The ability to run the trading system*

There are zero external service dependencies;
so, TSS Integration can run the system anywhere.

- Core
- Colo
- A single homeserver
- The pre-push test farm

*The ability to run the trading system*

- Makes testing pretty easy, actually
- More on this later

So forget about testing for a moment.

Our goal right now:
*The ability to run the trading system*
(in a flexible, robust, portable, self-contained way)

So how do we run the system?

There is no common service framework.

- Many different teams producing many different daemons
- C++, C, Java, Python, etc
- Configuration: Properties, CSVs, YAML, environment variables
- Protocols: Protocolgen, Protobuf, Object Channel, HTTP, etc
- Transports: UDP, TCP, mqueue, iqueue, msgbox, etc
- Etc!

Our solution has to handle all this.

Plus we have the normal problems!

- service discovery,
- distributed execution,
- process management,
- artifact deployment,
- etc...

And we need to stay portable and avoid external service dependencies.

We need to run in:

- Core
- Colo
- A single homeserver
- The pre-push test farm

So what do we do?

It's not so bad actually.
We can solve this with... a type system.

(See http://catern.com/progsys.html)

A type system!

A basic one, the one your language probably already has

No fancy dependent types, nothing unusual

A basic example problem:

Service A connects to services B and C.

Immediate consequences:
- B and C need to start before A
- A needs to know the URLs of B and C

Basically a problem of dependency injection...


[[file:tweet.png]]

** You can express dependencies between services using function arguments.

#+begin_src python
def start_a(b: B, c: C) -> A:
    ...
    start_process([
      "/bin/a",
      "--b-url", b.url,
      "--c-url", c.url,
      ...
    ])
    ...
    return A(...)
#+end_src

=start_a= takes two arguments, of types =B= and =C=,
and returns a value of type =A=.
Internally, it starts up service A,
configuring A using details from the function arguments.

If you want to start service A,
you've got create instances of =B= and =C= first;
presumably by starting services B and C!
That's exactly the invariant we want to enforce.

TSS Integration defines such types and functions,
for all the services and components in the trading system.

These types include all the details required to communicate with the service.

** You can keep track of complex values using types.
Service D can listen on either an HTTP or HTTP2 URL.
Service E can only work with service D if it's in HTTP2 mode.

#+begin_src python
def start_e(d: D[HTTP2Url]) -> E:
    ...
      "--d-url", d.url, # an HTTP2Url
    ...
    return E(...)


def main(d: D[HTTPUrl]) -> E:
    # type error!
    return start_e(d)
#+end_src

The =D= class takes a type argument specifying the type of =d.url=.
(Like Java generics or C++ templates)
** You can create different environments by passing different arguments to functions.
A third and final example.

For different use cases, we'll want systems configured in different ways.

Service F has an optional dependency on service G;
F can run whether or not service G is available.

    #+begin_src python
    def start_f(g: Optional[G]) -> F:
        ...
        if g:
           ... "--g-url", g.url ...
        else:
           pass
        return F(...)

    def environment_one() -> None:
        g = ...
        f = start_f(g)
        ...


    def environment_two() -> None:
        f = start_f(None)
        ...
    #+end_src

We can use =environment_one= or =environment_two=, each where appropriate.
(See http://catern.com/config.html for more general discussion of this specific technique)

In TSS Integration, the same functions and types are used for both production and testing.

But different environments are built out of those components,
using technique we just saw.

*** In-depth example of writing a test with TSS Integration
We'll look at TPS, the Two sigma Position Service.

What it actually does is not relevant, but...

- It's an order gateway
- Receives orders from the a mysterious source in the outside world
- Validates and possibly rejects those orders
- Updates the TS position daemon, =posdelta=, with the fills (if the order isn't rejected)

Why TPS?

- The subsystem required to support TPS is relatively small.
- TPS is run by TSS Integration in production.

This example is not meant to be practically useful.

Goal:
See how the principles we just discussed are put into practice.

The tests starts in the normal way for a Python test:
With a =unittest.TestCase=.

#+begin_src python
from integration.tps import start_tps
from unittest import TestCase

class TestTPS(TestCase):
    def setUp(self) -> None:
        self.tps = start_tps(...)

    def test(self) -> None:
        self.assertTrue("Do test stuff")
#+end_src

Normally, a new test for TPS would go in the existing file of TPS tests,
which already sets up a test system that includes TPS,
and we wouldn't have to write any setup code.
But for the sake of this example,
we'll pretend that we're writing the first tests for TPS,
and we have to set up the environment from scratch.

We'll start up TPS in =setUp= by calling =start_tps=,
and perform the actual test in =test=.

Now, let's look at the actual signature of =start_tps=.

#+begin_src python
async def start_tps(
  nursery: trio.Nursery,
  thread: Thread,
  listening_sock: FileDescriptor,
  database: Database,
  posdelta: Posdelta,
  publishing_iqueue: PassthroughPublishingIqueue,
  static_data: StaticData,
) -> TPS:
  ...
#+end_src

Don't panic! Yes, it has 7 arguments, meaning 7 dependencies,
but we'll be able to create them all quickly.

First, some boilerplate.
This is an =async= function,
so we'll need to call it from an async context.
We use the open source =trio= library for async Python,
so to get the test into an async context, we'll switch =TestCase= to =TrioTestCase=.

#+begin_src python
from integration.tps import start_tps
from trio import TrioTestCase

class TestTPS(TrioTestCase):
    async def asyncSetUp(self) -> None:
        self.tps = await start_tps(...)

    async def test(self) -> None:
        self.assertTrue("Do test stuff")
#+end_src

The first two arguments are also boilerplate.
The first argument is =nursery: trio.Nursery=,
which is used to start up background asynchronous tasks and detect if they fail.

In this case, if anything fails, we just want to fail the test,
so =TrioTestCase= helps here,
because it comes with a =trio.Nursery= ready-to-use, so we'll just use that.

#+begin_src python
        self.tps = await start_tps(
            self.nursery,
            ...,
        )
#+end_src

The second argument is a =Thread= from the open source =rsyscall= library,
which lets us start up threads on remote hosts and manipulate them,
which is how we distribute execution across multiple machines.
This test, we just want to run locally, so we'll just use =local_thread=;
we'll assign it to =self.thread= for easy access.

#+begin_src python
from rsyscall import local_thread

        self.thread = local_thread
        self.tps = await start_tps(
            self.thread,
            ...,
        )
#+end_src

Now for the actual dependencies.
=listening_sock: FileDescriptor=
The TPS daemon wants a pre-created listening socket, which it will use to listen for incoming connections.
So this is just some pretty standard Unix socket programming.
We'll create the socket that TPS wants here, and pass it down to TPS child process.
A TCP socket will work fine, so we'll specify AF_INET and SOCK_STREAM to get a TCP socket,
and bind it to a random unused port by specifying port 0.

#+begin_src python
from rsyscall.socket import AF, SOCK
from rsyscall.netinet.in_ import SockaddrIn

        sock = await self.thread.socket(AF.INET, SOCK.STREAM)
        await sock.bind(await self.thread.ptr(SockaddrIn(0, "127.0.0.1")))
        await sock.listen(1024)
        self.tps = await start_tps(
            ...,
            sock,
            ...,
        )
#+end_src

Now we need to connect TPS to the database it uses as a backend and persistence mechanism.
SQLite will work fine for us, since this is just a test.
This =Database= type is really =integration.tps.Database=,
and it comes with a method to initialize the TPS database schema,
so we'll just call that.

We do need a path to put the database at;
we'll use a helper function, =make_location=,
which makes a temporary directory at an appropriate local path.
This is frequently the first thing in any setUp method.

We usually call =make_location= just once, then make further files and directories under the path it returns.

We use =self.thread= again,
to indicate that the database should be created on the same host as everything else.

#+begin_src python
from integration.lib.utils import make_location

        self.location = make_location()
        self.tps = await start_tps(
            ...,
            await Database.make(self.thread, self.location/"tps.db"),
            ...,
        )
#+end_src

Now for =posdelta=, which is a service which distributes position updates to many clients.
We need to start up =posdelta=, which we do, naturally, with =start_posdelta=.
Let's look at the interface for start_posdelta...

#+begin_src python
async def start_posdelta(
    nursery: trio.Nursery, workdir: Path,
    static_data: StaticData=None,
    cpu: cpuset.Cpu=None, order_map_size: int=2048,
    filters: t.List[IqsyncFilter]=[],
) -> Posdelta:
#+end_src

A lot of arguments, but most of them are optional.
We only actually need =nursery: trio.Nursery= and =workdir: Path=.
We already have a =trio.Nursery=.
=workdir= is a relatively common parameter,
a directory that the service will use to store its state.
We can just make a directory under =self.location=.
So:

#+begin_src python
from integration.posdelta import make_location

        self.posdelta = await start_posdelta(self.nursery, (self.location/"posdelta").mkdir())
        self.tps = await start_tps(
            ...,
            self.posdelta,
            ...,
        )
#+end_src

OK, so we started up the posdelta service, now back to TPS.

Next is =publishing_iqueue: PassthroughPublishingIqueue=.
A =PassthroughPublishingIqueue= is a specific type of iqueue
used by several different services (not just TPS) to publish positions to posdelta.
Let's look at the constructor.

#+begin_src python
class PassthroughPublishingIqueue:
    @staticmethod
    async def make(
        thread: Thread,
        path: Path,
        src_id: int=0,
    ) -> PassthroughPublishingIqueue: ...
#+end_src

Again we need a =Thread=, to identify which host to operate on,
and a =Path=, to identify the path at which the iqueue should be created.
The last argument lets us customize =src_id=, but that's not something we care about for these tests.

#+begin_src python
from integration.tps import PassthroughPublishingIqueue

        self.tps = await start_tps(
            ...,
            await PassthroughPublishingIqueue.make(self.thread, self.location/"tps.iqx")
            ...,
        )
#+end_src

Almost there!
We need a =StaticData=,
which is a widely used type which contains a bunch of instrument reference data,
stored as files in a directory on disk.

Again, simple enough to make:
#+begin_src python
from integration.static_data import StaticData

        self.static_data = await StaticData.make(self.thread, (self.location/"static_data").mkdir())
        self.tps = await start_tps(
            ...,
            self.static_data,
            ...)
#+end_src

OK! We made it!

#+begin_src python
from integration.static_data import StaticData
from integration.tps import start_tps
from trio import TrioTestCase
from rsyscall import local_thread
from rsyscall.socket import AF, SOCK
from rsyscall.netinet.in_ import SockaddrIn

class TestTPS(TrioTestCase):
    async def asyncSetUp(self) -> None:
        self.thread = local_thread
        sock = await self.thread.socket(AF.INET, SOCK.STREAM)
        await sock.bind(await self.thread.ptr(SockaddrIn(0, "127.0.0.1")))
        await sock.listen(1024)
        self.static_data = await StaticData.make(self.thread, (self.location/"static_data").mkdir()),
        self.tps = await start_tps(
            self.nursery,
            self.thread,
            sock,
            await Database.make(self.thread, self.location/"tps.db"),
            await start_posdelta(self.nursery, (self.location/"posdelta").mkdir()),
            await PassthroughPublishingIqueue.make(self.thread, self.location/"tps.iqx"),
            self.static_data,
        )

    async def test(self) -> None:
        self.assertTrue("Do test stuff")
#+end_src

Now we'll use the Python client for TPS,
which connects to TPS's listening socket,
to send an order and a fill through,
in an arbitrary instrument.

#+begin_src python
    async def test(self) -> None:
        client = await self.tps.make_client()
        order = await client.new_order('buy', 100, self.static_data.instruments[0], Decimal('50.0'))
        await order.fill(100, Decimal('50.0')
#+end_src

If this succeeds without throwing an exception...
it means TPS is responding to requests correctly.
That's a test!
Now, of course, there are a few more features;
we'd probably want to send some sell orders,
and cancels, etc.

But once we have a running system,
all it takes to test it is to fire up the client we use in production,
and send some orders just like we do in prod.

Of course, we'd still want to show that it's interacting with other components correctly.
That's easy too!
The only way TPS interacts with other components is by sending position updates,
so we just fire up those other components
and check that the positions they report match the activity we've performed through TPS.

We don't need to,
and shouldn't,
look at the actual data TPS is sending out.

That's far too easy to get wrong.

Instead, we do our testing end to end.
We let the other services read that data,
and if what they report, and how they behave,
matches our expectations...
then we've passed the test.

This makes our testing simpler, less fragile, and more powerful;
we get full coverage for free, because it's the normal practice for testing everything.
* 
demonstrate the problem at the beginning

talk about why we can't use alternatives
** things I talked about the end
what we had before (mocks, stuff)

product management - people who use it

give context up front

(my thought: I should probably explain the use of Python typing and Python async up front)

yeah giving context up front makes sense actually, for sure.

obviously I'm jumping right in without context,
and explaining, like...

hey, we want to be able to... test things...
and...
what even is this library we're talking about...

yeah so giving a short history and description of the project management of the library might be useful.
